# -*- coding: utf-8 -*-
"""Submission2_MLTerapan_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_MTrhgfPhLhuplgSNa6FbcNsnwU9EjOb

# **Sistem Rekomendasi Buku - Submission Machine Learning Terapan**
Nama : Auliya Sabrina Vyantika

# **Import Library**
"""

!pip install opendatasets
import opendatasets as od
import pandas as pd
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import re

"""## **1. Data Understanding**
Dataset yang digunakan dalam proyek machine learning ini adalah Book Recommendation System 📚📚 yang diperoleh dari situs Kaggle. Dataset tersebut dapat diakses melalui tautan berikut: [Book Recommendation System 📚📚](https://www.kaggle.com/code/fahadmehfoooz/book-recommendation-system)
"""

# Load dataset
books = pd.read_csv('/content/Books.csv')
ratings = pd.read_csv('/content/Ratings.csv')
users = pd.read_csv('/content/Users.csv')

# Informasi jumlah data unik
print('Jumlah buku unik:', books['ISBN'].nunique())
print('Jumlah pengguna unik yang memberikan rating:', ratings['User-ID'].nunique())
print('Jumlah pengguna yang terdaftar:', users['User-ID'].nunique())

"""**Books Variabel**

Eksplorasi variabel books merupakan langkah awal untuk memahami struktur dan informasi yang tersedia dalam dataset buku. Dataset ini memuat informasi terkait buku yang digunakan dalam sistem rekomendasi.
"""

books.info()

"""Adapun beberapa kolom utama yang terdapat pada dataset ini adalah sebagai berikut:

- **ISBN**: Nomor identifikasi unik untuk setiap buku.  
- **Book-Title**: Judul dari buku.  
- **Book-Author**: Nama penulis buku.  
- **Year-Of-Publication**: Tahun terbit buku.  
- **Publisher**: Nama penerbit buku.  
- **Image-URL-S**, **Image-URL-M**, **Image-URL-L**: Tautan gambar sampul buku dalam ukuran kecil (small), sedang (medium), dan besar (large).

"""

# Menampilkan 5 data teratas
books.head()

# Melihat jumlah total data
print("Jumlah total data buku:", books.shape[0])

# Melihat jumlah nilai unik
print("\nJumlah buku unik berdasarkan ISBN:", books['ISBN'].nunique())
print("Jumlah penulis unik:", books['Book-Author'].nunique())
print("Jumlah penerbit unik:", books['Publisher'].nunique())

# Cek nilai yang hilang
print("\nJumlah missing value tiap kolom:")
print(books.isnull().sum())

"""**Ratings Variabel**

Eksplorasi variabel ratings bertujuan untuk memahami data yang berkaitan dengan penilaian (rating) yang diberikan oleh pengguna terhadap buku.
"""

ratings.head()

ratings.info()

"""Berikut penjelasan setiap kolom dalam dataset ratings:

- **User-ID**: Identitas unik pengguna yang memberikan rating.  
- **ISBN**: Nomor identifikasi buku yang diberi rating.  
- **Book-Rating**: Nilai rating yang diberikan oleh pengguna terhadap buku, biasanya dalam skala 0–10.

"""

ratings.describe()

"""Hasil eksplorasi statistik deskriptif pada dataset `ratings` menunjukkan bahwa:

- Terdapat **1.149.780 data rating** yang diberikan oleh pengguna.
- **Rata-rata rating (mean)** yang diberikan adalah sekitar **2.87**, dengan **standar deviasi** sebesar **3.85**, yang menunjukkan penyebaran rating yang cukup luas.
- **Rating minimum** adalah **0**, yang biasanya menandakan **tidak adanya rating eksplisit** dari pengguna.
- **Rating maksimum** adalah **10**, yang merupakan skor tertinggi yang bisa diberikan pada sebuah buku.
- Kuartil ke-1 (25%), ke-2 (median/50%), dan ke-3 (75%) masing-masing bernilai **0**, **0**, dan **7**, mengindikasikan bahwa sebagian besar data rating yang ada adalah **0**, sehingga hanya sebagian kecil pengguna yang memberikan rating eksplisit terhadap buku.

**Kesimpulan**: Sebagian besar entri pada kolom `Book-Rating` bernilai 0, yang berarti banyak interaksi pengguna tidak disertai dengan rating eksplisit. Oleh karena itu, untuk membangun sistem rekomendasi berbasis rating, disarankan untuk memfilter hanya data dengan `Book-Rating > 0`.
"""

# Menampilkan 5 data teratas
ratings.head()

# Melihat jumlah total data
print("Jumlah total data rating:", ratings.shape[0])

# Melihat jumlah nilai unik
print("\nJumlah pengguna unik yang memberikan rating:", ratings['User-ID'].nunique())
print("Jumlah buku unik yang diberi rating:", ratings['ISBN'].nunique())

# Cek nilai yang hilang
print("\nJumlah missing value tiap kolom:")
print(ratings.isnull().sum())

"""**Users**

Eksplorasi data `users` merupakan langkah awal untuk memahami karakteristik pengguna dalam sistem rekomendasi.

"""

users.head()

users.info()

# Statistik deskriptif kolom numerik (Age)
print("\nStatistik deskriptif kolom 'Age':")
print(users['Age'].describe())

# Melihat jumlah pengguna unik
print("\nJumlah pengguna unik:")
print(users['User-ID'].nunique())

# Melihat jumlah lokasi unik
print("\nJumlah lokasi unik:")
print(users['Location'].nunique())

"""Hasil eksplorasi terhadap kolom `Age` pada dataset `users` menunjukkan beberapa hal penting:

- Terdapat **168.096** pengguna yang memiliki data usia.
- **Rata-rata usia** pengguna adalah sekitar **34,75 tahun** dengan standar deviasi **14,43**, yang menunjukkan sebaran usia cukup luas.
- Nilai usia minimum adalah **0 tahun** dan maksimum mencapai **244 tahun**, yang jelas merupakan nilai tidak valid atau outlier.
- Kuartil ke-1, ke-2 (median), dan ke-3 masing-masing berada di usia **24**, **32**, dan **44 tahun**.

**Kesimpulan**: Terdapat data usia yang tidak realistis (seperti 0 tahun dan 244 tahun). Oleh karena itu, perlu dilakukan pembersihan data dengan menyaring hanya pengguna yang berusia antara **5 hingga 100 tahun**, yang dianggap sebagai rentang usia yang masuk akal.

Selain itu:
- Dataset memuat **278.858 pengguna unik**.
- Terdapat **57.339 lokasi unik**, yang menunjukkan keberagaman geografis yang luas dari pengguna dalam sistem rekomendasi ini.

## **2. Data Preprocessing**

### **Menggabungkan Semua Data**

Proses data preprocessing dimulai dengan menggabungkan ketiga file utama, yaitu `Books.csv`, `Users.csv`, dan `Ratings.csv`. Penggabungan dilakukan untuk membentuk satu dataset yang utuh, sehingga dapat digunakan dalam proses analisis dan pemodelan sistem rekomendasi.

Penggabungan dilakukan berdasarkan kolom **`ISBN`** untuk menggabungkan `Books` dan `Ratings`, serta kolom **`User-ID`** untuk menggabungkan dengan `Users`. Dengan langkah ini, setiap entri rating akan memiliki informasi lengkap mengenai buku yang dinilai dan pengguna yang memberi penilaian.
"""

# Menggabungkan seluruh ISBN pada kategori books dan ratings
isbn_all = np.concatenate((
    books['ISBN'].unique(),
    ratings['ISBN'].unique()
))

# Mengurutkan data dan menghapus data yang sama
isbn_all = np.sort(np.unique(isbn_all))

# Menggabungkan seluruh User-ID pada kategori ratings dan users
user_all = np.concatenate((
    ratings['User-ID'].unique(),
    users['User-ID'].unique()
))

# Mengurutkan data User-ID dan menghapus duplikasi
user_all = np.sort(np.unique(user_all))

# Menampilkan hasilnya
print('Jumlah seluruh data ISBN berdasarkan ISBN: ', len(isbn_all))
print('Jumlah seluruh data pengguna berdasarkan User-ID: ', len(user_all))

# Menggabungkan file books dan ratings ke dalam dataframe book_rating
book_rating = pd.merge(ratings, books, on='ISBN', how='left')

# Menggabungkan dataframe book_rating dengan users berdasarkan User-ID
data = pd.merge(book_rating, users, on='User-ID', how='left')

# Menampilkan hasil
print('Jumlah data setelah penggabungan: ', data.shape)
data.head()

data.info()

"""## Penggabungan Data

Sekarang, semua file yang saya miliki telah digabungkan menjadi satu dataset. Dataset ini menggabungkan informasi mengenai rating buku, detail buku, dan data pengguna. Dataset akhir memiliki **1.149.780 baris** dan **12 kolom**, yang meliputi informasi seperti `User-ID`, `ISBN`, `Book-Rating`, `Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`, `Location`, dan `Age`.

Meskipun demikian, terdapat beberapa kolom yang memiliki nilai **missing** (null), seperti `Book-Title`, `Book-Author`, `Publisher`, dan `Image-URL`, yang perlu ditangani lebih lanjut dalam tahap preprocessing untuk memastikan kelengkapan data.

## **3. Data Preparation**
"""

data.isnull().sum()

"""**Hasil Analisis Missing Values**

Hasil analisis nilai yang hilang (missing values) pada dataset menunjukkan bahwa ada beberapa kolom yang memiliki nilai **null**:

- **Book-Title**, **Book-Author**, **Year-Of-Publication**, **Publisher**, dan gambar (kolom **Image-URL-S**, **Image-URL-M**, dan **Image-URL-L**) memiliki jumlah nilai null yang cukup signifikan, masing-masing sekitar **118.644 hingga 118.648 baris**.
- Kolom **Age** juga memiliki **309.492 nilai null**.
- Sementara itu, kolom **User-ID**, **ISBN**, **Book-Rating**, dan **Location** tidak memiliki nilai null sama sekali, yang berarti data pada kolom-kolom ini lengkap.

Dengan demikian, langkah selanjutnya dalam tahap preprocessing adalah menangani nilai-nilai yang hilang pada kolom-kolom tersebut, baik dengan menghapus baris yang memiliki nilai null atau mengisi nilai tersebut dengan estimasi yang sesuai.

"""

# Membersihkan missing value dengan fungsi dropna()
data_clean = data.dropna()
data_clean.head(3)

# Mengecek kembali missing value pada variabel data_clean
data_clean.isnull().sum()

data_clean.describe()

# Memfilter data umur agar hanya mencakup 5-99 tahun
data_clean = data_clean[(data_clean['Age'] >= 5) & (data_clean['Age'] <= 99)]

# Mengecek deskripsi statistik setelah perbaikan
data_clean['Age'].describe()

# Mengurutkan data berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_data
data_clean = data_clean.sort_values('ISBN', ascending=True)

# Mengecek kategori penulis unik
data_clean['Book-Author'].unique()
data_clean.head()

# Membuat variabel preparation yang berisi dataframe data_clean kemudian mengurutkan berdasarkan placeID
preparation = data_clean
preparation.sort_values('ISBN')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation.head()

preparation.isnull().sum()

# Mengonversi kolom 'ISBN' menjadi list
book_id = preparation['ISBN'].tolist()

# Mengonversi kolom 'Book-Title' menjadi list
book_title = preparation['Book-Title'].tolist()

# Mengonversi kolom 'Book-Author' menjadi list
book_author = preparation['Book-Author'].tolist()

# Menampilkan panjang masing-masing list
print(len(book_id))
print(len(book_title))
print(len(book_author))

# Membuat dictionary untuk data 'ISBN', 'Book-Title', dan 'Book-Author'
book_new = pd.DataFrame({
    'id': book_id,
    'book_title': book_title,
    'author': book_author
})

book_new

"""## **4. Modeling and Result**

Proses modeling yang saya lakukan pada data ini adalah dengan menerapkan dua pendekatan algoritma machine learning, yaitu **Content-Based Filtering** dan **Collaborative Filtering**.

Untuk algoritma **Content-Based Filtering**, saya membangun sistem rekomendasi berdasarkan informasi konten dari buku, seperti *judul buku* dan *penulis*, dengan mengasumsikan bahwa pengguna akan tertarik pada item yang mirip dengan item yang disukainya di masa lalu.

Sedangkan untuk algoritma **Collaborative Filtering**, saya memanfaatkan data *rating* dari pengguna lain untuk memberikan rekomendasi, dengan asumsi bahwa jika dua pengguna memiliki preferensi yang mirip, maka buku yang disukai oleh satu pengguna kemungkinan juga disukai oleh yang lain.

"""

data_fix = book_new
data_fix.sample(5)

data_fix.sort_values(by='author', ascending=True)

# Fungsi untuk menghapus karakter aneh pada kolom 'author'
def clean_author_names(df):
    df['author'] = df['author'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', x))
    return df

# Terapkan fungsi untuk membersihkan data_fix
data_fix = clean_author_names(data_fix)

# Tampilkan hasil setelah pembersihan
data_fix.head()

# Salin data awal
filtered_data = data_fix.copy()

# Fungsi untuk memastikan nama author memiliki minimal satu kata
def is_valid_author(name):
    # Mengabaikan author dengan karakter aneh sudah dilakukan sebelumnya
    return len(name.split()) >= 1

# Terapkan filter ke kolom 'author' untuk memastikan hanya author valid
filtered_data = filtered_data[filtered_data['author'].apply(is_valid_author)]

# Reset index (opsional)
filtered_data = filtered_data.reset_index(drop=True)

# Gabungkan nama author menjadi satu tanpa spasi dan huruf kecil semua
filtered_data['author'] = filtered_data['author'].apply(lambda x: ''.join(x.split()).lower())

# Tampilkan hasil
filtered_data

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data author
tf.fit(filtered_data['author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(filtered_data['author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan author
# Baris diisi dengan judul buku

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=filtered_data.book_title
).sample(22, axis=1).sample(10, axis=0)

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=filtered_data['book_title'], columns=filtered_data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## **5. Mendapatkan Rekomendasi**

### 📚 Fungsi `book_recommendations`

Pada bagian ini, kita membuat fungsi `book_recommendations` dengan beberapa parameter sebagai berikut:

- **book_title** : Judul buku yang dijadikan sebagai acuan untuk mencari buku-buku lain yang mirip.
- **similarity_data** : Matriks kemiripan yang telah dihitung sebelumnya, misalnya menggunakan TF-IDF.
- **items** : DataFrame yang berisi informasi buku seperti `book_title` dan `author`, yang digunakan untuk mencocokkan indeks dengan data asli.
- **k** : Jumlah rekomendasi buku yang ingin ditampilkan berdasarkan tingkat kemiripan tertinggi terhadap buku acuan.

Fungsi ini akan menghasilkan daftar rekomendasi buku yang memiliki kemiripan konten dengan buku yang dipilih.
  



"""

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=data_fix[['book_title', 'author']], k=5):
    """
    Rekomendasi Buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    book_title : tipe data string (str)
                 Judul buku sebagai index acuan dari dataframe kemiripan
    similarity_data : tipe data pd.DataFrame (object)
                      DataFrame kemiripan antar judul buku, bersifat simetrik
    items : tipe data pd.DataFrame (object)
            DataFrame yang memuat kolom 'book_title' dan fitur lain seperti 'author'
    k : tipe data integer (int)
        Jumlah rekomendasi buku yang diinginkan
    ---

    Fungsi ini mengambil nilai similarity terbesar berdasarkan judul buku
    yang diberikan, kemudian mengembalikan daftar rekomendasi buku lainnya
    yang paling mirip.
    """

    # Mengambil index kemiripan terbesar dari array numpy
    index = similarity_data.loc[:, book_title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil nama-nama buku dengan similarity tertinggi
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Menghapus judul buku awal agar tidak direkomendasikan ke dirinya sendiri
    closest = closest.drop(book_title, errors='ignore')

    # Menggabungkan hasil dengan data asli untuk mendapatkan info tambahan
    recommendations = pd.DataFrame(closest).merge(items)

    # Menghapus duplikat berdasarkan 'book_title' dan 'author'
    recommendations_unique = recommendations.drop_duplicates(subset=['book_title', 'author'])

    # Mengembalikan hasil dengan jumlah rekomendasi yang diinginkan
    return recommendations_unique.head(k)

# Mengelompokkan berdasarkan kolom 'author' dan menghitung jumlah kemunculannya
author_counts = data_fix.groupby('author').size().reset_index(name='jumlah_buku')

# Mengurutkan berdasarkan jumlah buku dari yang terbanyak
author_counts_sorted = author_counts.sort_values(by='jumlah_buku', ascending=False)

# Menampilkan hasil
print(author_counts_sorted)

data_fix[data_fix.book_title.eq('Passenger to Frankfurt: An extravaganza,')]

# Mendapatkan rekomendasi book yang mirip
book_recommendations('Passenger to Frankfurt: An extravaganza,')

"""Berdasarkan input buku berjudul **Hickory Dickory Dock (Hercule Poirot Mysteries...)** karya *Agatha Christie*,  
sistem merekomendasikan buku-buku lain yang memiliki kemiripan berdasarkan analisis konten judul dan penulisnya.  

Sebagian besar rekomendasi merupakan karya dari penulis yang sama, yaitu *Agatha Christie*,  
yang menunjukkan bahwa sistem mengenali kesamaan dalam gaya penulisan dan tema cerita berdasarkan pola dalam data.  Ini dapat membantu pengguna menemukan buku lain dari penulis favoritnya atau buku yang memiliki karakteristik serupa.

# **1) Model Development dengan Collaborative Filtering**
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

# Membaca dataset

df = ratings
df

"""## **2) Data Preparation**"""

# Mengubah User-ID menjadi list unik
user_ids = ratings['User-ID'].unique().tolist()
print('List userID:', user_ids)

# Melakukan encoding User-ID (User-ID → angka)
user_to_user_encoded = {user_id: idx for idx, user_id in enumerate(user_ids)}
print('Encoded User-ID:', user_to_user_encoded)

# Membuat mapping sebaliknya (angka → User-ID)
user_encoded_to_user = {idx: user_id for idx, user_id in enumerate(user_ids)}
print('Encoded angka ke User-ID:', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = ratings['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
isbn_to_encoded = {x: i for i, x in enumerate(isbn_list)}

# Melakukan proses encoding angka ke ISBN
encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

# Mapping User-ID ke kolom 'user' dalam dataframe ratings
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke kolom 'book' dalam dataframe ratings
ratings['book'] = ratings['ISBN'].map(isbn_to_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_books = len(encoded_to_isbn)
print(num_books)

# Mengubah rating menjadi nilai float
ratings['rating'] = ratings['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = ratings['rating'].min()

# Nilai maksimal rating
max_rating = ratings['rating'].max()

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""**Membagi Data untuk Training dan Validasi**

"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = ratings[['user', 'book']].values

# Membuat variabel y dengan normalisasi rating
y = ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):

  # Inisialisasi fungsi
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

    self.book_embedding = layers.Embedding( # layer embedding book
        num_books,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_books, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])     # embedding user
    user_bias = self.user_bias(inputs[:, 0])             # bias user
    book_vector = self.book_embedding(inputs[:, 1])      # embedding book
    book_bias = self.book_bias(inputs[:, 1])             # bias book

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)  # dot product

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x)  # sigmoid activation

"""## Evaluation
Selanjutnya, lakukan proses compile terhadap model. serta menggunakan matrix evaluasi RMSE


"""

# Inisialisasi model dengan jumlah user dan buku
model = RecommenderNet(num_users, num_books, 50)

# Compile model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Memulai proses training dengan batch size sebesar 64 serta epoch 100 kali"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""**Visualisasi Metrik**  
Untuk melihat visualisasi proses training
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Evaluasi Visualisasi Training Model**

Dari visualisasi proses training model di atas, terlihat bahwa proses pelatihan berlangsung dengan cukup mulus, meskipun terdapat sedikit fluktuasi pada awal epoch. Secara keseluruhan, model menunjukkan kecenderungan konvergen hingga mencapai epoch ke-100.

Berdasarkan hasil training tersebut, diperoleh nilai *root mean squared error* akhir pada data training sebesar kurang lebih **0.55**, sedangkan pada data validasi mencapai sekitar **0.49**. Hal ini menunjukkan adanya sedikit perbedaan performa antara data training dan validasi, yang mengindikasikan potensi *overfitting* pada model.

***Mendapatkan Rekomendasi Buku***
"""

books.rename(columns={
    'Book-Title': 'Book_Title',
    'Book-Author': 'Book_Author'
}, inplace=True)

book_df = books
df = ratings

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_rated_by_user = df[df['User-ID'] == user_id]

# Menentukan buku yang belum dibaca oleh user tersebut
book_not_rated = book_df[~book_df['ISBN'].isin(book_rated_by_user['ISBN'].values)]['ISBN']
book_not_rated = list(
    set(book_not_rated)
    .intersection(set(isbn_to_encoded.keys()))
)

# Ubah ke format encoded
book_not_rated = [[isbn_to_encoded.get(x)] for x in book_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_rated), book_not_rated)
)

# Ambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_rated_by_user = df[df['User-ID'] == user_id]

# Tentukan buku yang belum dibaca oleh user tersebut
book_not_rated = book_df[~book_df['ISBN'].isin(book_rated_by_user['ISBN'].values)]['ISBN']
book_not_rated = list(set(book_not_rated).intersection(set(isbn_to_encoded.keys())))

# Ubah ke format encoded
book_not_rated = [[isbn_to_encoded.get(x)] for x in book_not_rated]
user_encoder = user_to_user_encoded.get(user_id)

# Gabungkan user dan buku yang belum dibaca
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_rated), book_not_rated)
)

# Prediksi rating untuk buku yang belum dibaca
ratings = model.predict(user_book_array).flatten()

# Ambil indeks 10 buku dengan prediksi rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Ambil ISBN dari buku yang direkomendasikan
recommended_books = [encoded_to_isbn.get(book_not_rated[x][0]) for x in top_ratings_indices]

# Tampilkan rekomendasi
print('Showing recommendations for user: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

# Tampilkan 5 buku yang pernah diberi rating tertinggi oleh user
top_books_user = (
    book_rated_by_user.sort_values(by='rating', ascending=False)
    .head(5)
    .ISBN.values
)
book_df_rows = book_df[book_df['ISBN'].isin(top_books_user)]

for row in book_df_rows.itertuples():
    print(row.Book_Title, ':', row.Book_Author)

print('----' * 8)
print('Top 10 book recommendations')
print('----' * 8)

# Tampilkan 10 buku hasil rekomendasi
recommended_books_df = book_df[book_df['ISBN'].isin(recommended_books)]
for row in recommended_books_df.itertuples():
    print(row.Book_Title, ':', row.Book_Author)

"""## 📚 INSIGHT SISTEM REKOMENDASI BUKU UNTUK USER ID 77383

### ✅ 1. Preferensi Awal User Sangat Jelas (Fantasi & Fiksi Ilmiah)
Pengguna 77383 memberikan rating tinggi pada buku-buku bergenre:
- **Fantasi epik & fiksi ilmiah**, seperti:
  - *Bearing an Hourglass* — Piers Anthony  
  - *The Soprano Sorceress* — L. E. Modesitt Jr.  
  - *Prophecy* — Elizabeth Haydon

➡️ Menunjukkan bahwa model berhasil **menangkap pola kesukaan user** berdasarkan histori pembacaannya.

---

### ✅ 2. Rekomendasi Model Variatif namun Masih Relevan
Model merekomendasikan buku dengan **genre yang beragam**, antara lain:
- **Fiksi ilmiah dan cyberpunk**: *Snow Crash*  
- **Misteri klasik**: *N or M?* oleh Agatha Christie  
- **Literatur anak-anak**: *If You Give a Mouse a Cookie*  
- **Puisi klasik**: *100 Selected Poems by E. E. Cummings*  
- **Spiritualitas & kehidupan holistik**: *Earthway*, *Poussières d’étoiles*  
- **Psikologi & biografi**: *First Person Plural*

➡️ Memberikan ruang eksplorasi bacaan sambil tetap relevan dengan minat user.

---

### ✅ 3. Keseimbangan antara Familiar dan Novel
Beberapa buku seperti *Snow Crash* masih berdekatan dengan genre yang disukai user, sedangkan buku lain memperluas pengalaman membaca.

➡️ Menunjukkan kemampuan model untuk memberikan **kombinasi yang seimbang** antara kenyamanan dan tantangan eksploratif.

---

### ✅ 4. Tidak Ada Duplikasi Buku Lama
Sistem memastikan bahwa **rekomendasi adalah buku-buku baru** bagi user tersebut, yang belum pernah diberi rating.

➡️ Ini menjaga **relevansi dan nilai tambah** dari rekomendasi yang diberikan.

---

### ✅ 5. Indikasi Akurasi dan Performa Model Baik
Model menyelesaikan prediksi terhadap 8439 buku dalam waktu **14 detik**, dan hasilnya logis serta bervariasi.

➡️ Menandakan bahwa model memiliki **akurasi dan efisiensi yang memadai** untuk implementasi real-time.

---

### 📌 Kesimpulan
Sistem rekomendasi telah berhasil:
- Menyesuaikan saran berdasarkan histori pengguna.
- Menawarkan kombinasi bacaan populer dan niche.
- Menghindari pengulangan bacaan.
- Meningkatkan potensi eksplorasi genre baru oleh pengguna.


"""